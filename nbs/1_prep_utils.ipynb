{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp prep_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Utils\n",
    "\n",
    "> Basic utility functions across ard-workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "from glob import glob\n",
    "import gc\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "import boto3\n",
    "import gdal\n",
    "from asynchronousfilereader import AsynchronousFileReader\n",
    "\n",
    "from sedas_pyapi.sedas_api import SeDASAPI\n",
    "from sedas_pyapi.bulk_download import SeDASBulkDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sedas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NovaSAR and other Catapult-hosted datasets, me must download from Sedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_client():\n",
    "    \"Log into sedas Test api.\"\n",
    "    sedas = SeDASAPI(os.getenv('SEDAS_USERNAME'), os.getenv('SEDAS_PWD'))\n",
    "    sedas.base_url=\"https://geobrowsertest.satapps.org/api/\"\n",
    "    \n",
    "    sedas.sensor_url = f\"{sedas.base_url}sensors\"\n",
    "    sedas.authentication_url = f\"{sedas.base_url}authentication\"\n",
    "    sedas.search_url = f\"{sedas.base_url}search\"\n",
    "    sedas._token = None  \n",
    "    # now we can get the users actual test password\n",
    "    sedas._username = 'tom_jones'\n",
    "    sedas.__password = os.getenv('SEDAS_PWD')\n",
    "    # and log into test\n",
    "    sedas.login()\n",
    "    \n",
    "    return sedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sedas_pyapi.sedas_api.SeDASAPI at 0x7fdf22b83890>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedas_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should inc. test for env vars...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll want to find datasets from different *Collections*, which sedas expects as *groups*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_get_collections():\n",
    "    sedas = sedas_client()\n",
    "    result_groups = sedas.list_sensor_groups()\n",
    "    groups = []\n",
    "    for i in range(0, len(result_groups)):\n",
    "        groups.append(result_groups[i]['name'])\n",
    "    return f\"Available groups are: {', '.join(groups)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Available groups are: Cosmo-SkyMed, SPOT, Pleiades, S1, S2, AIRSAR'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedas_get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_find_datasets(wkt, startDate, endDate, collection):\n",
    "    optical_groups = ['S2','Pleiades','SPOT']\n",
    "    sar_groups = ['S1','Cosmo-SkyMed','AIRSAR','NovaSAR']\n",
    "    sedas = sedas_client()\n",
    "    if collection in optical_groups: \n",
    "        res = sedas.search_optical(wkt, startDate, endDate, source_group=collection) \n",
    "    elif collection in sar_groups:\n",
    "        res = sedas.search_sar(wkt, startDate, endDate, source_group=collection)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example for a small area over Oxford we can find..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>supplierId</th>\n",
       "      <th>type</th>\n",
       "      <th>satelliteName</th>\n",
       "      <th>instrumentName</th>\n",
       "      <th>modeName</th>\n",
       "      <th>sensorType</th>\n",
       "      <th>sensorResolution</th>\n",
       "      <th>coordinatesWKT</th>\n",
       "      <th>start</th>\n",
       "      <th>...</th>\n",
       "      <th>area</th>\n",
       "      <th>aoiCoveragePercent</th>\n",
       "      <th>usefulAreaPercent</th>\n",
       "      <th>cloudCoveragePercent</th>\n",
       "      <th>productType</th>\n",
       "      <th>latency</th>\n",
       "      <th>ql</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>vendorSpecific</th>\n",
       "      <th>downloadUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0ec9a5f12356e87bc910ddbc49dbb76</td>\n",
       "      <td>Pleiades_UKSA396_SO18034616-96-01_DS_PHR1B_201...</td>\n",
       "      <td>ARCHIVE</td>\n",
       "      <td>Pleiades-1B</td>\n",
       "      <td>MS/PAN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Optical</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POLYGON((-1.654728 51.309517,-1.345414 51.3081...</td>\n",
       "      <td>2018-10-24T11:17:22Z</td>\n",
       "      <td>...</td>\n",
       "      <td>4.825125e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L3</td>\n",
       "      <td>Standard</td>\n",
       "      <td>https://geobrowser.satapps.org/archiveql/aeweb...</td>\n",
       "      <td>https://sedasdm.satapps.org/qls/qlmgr.php?scen...</td>\n",
       "      <td>{'property': 'vendorSpecific', 'Filehash': 'cf...</td>\n",
       "      <td>https://sedasdm.satapps.org/datamgr/datamgr.ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94cc7887414be0912de7ca44288f79da</td>\n",
       "      <td>Pleiades_UKSA174_SO18034614-74-01_DS_PHR1A_201...</td>\n",
       "      <td>ARCHIVE</td>\n",
       "      <td>Pleiades-1A</td>\n",
       "      <td>MS/PAN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Optical</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POLYGON((-2.153852 51.603313,-1.841533 51.6033...</td>\n",
       "      <td>2018-09-29T11:10:08Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1.565260e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L3</td>\n",
       "      <td>Standard</td>\n",
       "      <td>https://geobrowser.satapps.org/archiveql/aeweb...</td>\n",
       "      <td>https://sedasdm.satapps.org/qls/qlmgr.php?scen...</td>\n",
       "      <td>{'property': 'vendorSpecific', 'Filehash': '4b...</td>\n",
       "      <td>https://sedasdm.satapps.org/datamgr/datamgr.ph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          productId  \\\n",
       "0  c0ec9a5f12356e87bc910ddbc49dbb76   \n",
       "1  94cc7887414be0912de7ca44288f79da   \n",
       "\n",
       "                                          supplierId     type satelliteName  \\\n",
       "0  Pleiades_UKSA396_SO18034616-96-01_DS_PHR1B_201...  ARCHIVE   Pleiades-1B   \n",
       "1  Pleiades_UKSA174_SO18034614-74-01_DS_PHR1A_201...  ARCHIVE   Pleiades-1A   \n",
       "\n",
       "  instrumentName modeName sensorType  sensorResolution  \\\n",
       "0         MS/PAN    0.000    Optical               2.0   \n",
       "1         MS/PAN    0.000    Optical               2.0   \n",
       "\n",
       "                                      coordinatesWKT                 start  \\\n",
       "0  POLYGON((-1.654728 51.309517,-1.345414 51.3081...  2018-10-24T11:17:22Z   \n",
       "1  POLYGON((-2.153852 51.603313,-1.841533 51.6033...  2018-09-29T11:10:08Z   \n",
       "\n",
       "   ...          area aoiCoveragePercent  usefulAreaPercent  \\\n",
       "0  ...  4.825125e+08                1.0                5.0   \n",
       "1  ...  1.565260e+08                2.0               22.0   \n",
       "\n",
       "   cloudCoveragePercent  productType   latency  \\\n",
       "0                   0.0           L3  Standard   \n",
       "1                   0.0           L3  Standard   \n",
       "\n",
       "                                                  ql  \\\n",
       "0  https://geobrowser.satapps.org/archiveql/aeweb...   \n",
       "1  https://geobrowser.satapps.org/archiveql/aeweb...   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  https://sedasdm.satapps.org/qls/qlmgr.php?scen...   \n",
       "1  https://sedasdm.satapps.org/qls/qlmgr.php?scen...   \n",
       "\n",
       "                                      vendorSpecific  \\\n",
       "0  {'property': 'vendorSpecific', 'Filehash': 'cf...   \n",
       "1  {'property': 'vendorSpecific', 'Filehash': '4b...   \n",
       "\n",
       "                                         downloadUrl  \n",
       "0  https://sedasdm.satapps.org/datamgr/datamgr.ph...  \n",
       "1  https://sedasdm.satapps.org/datamgr/datamgr.ph...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sedas_find_datasets(\"POLYGON((-1.91 51.81,-1.15 51.81,-1.15 51.50,-1.91 51.50,-1.91 51.81))\", \n",
    "                             \"2000-01-01T00:00:00Z\", \n",
    "                             \"2020-10-27T00:00:00Z\",\n",
    "                             \"Pleiades\"\n",
    "                            )\n",
    "pd.DataFrame(result['products']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_download(sedas_res_dicts, down_dir, sedas=None):\n",
    "    \"Use product dicts list to download into a dir.\"\n",
    "    if not sedas:\n",
    "        sedas = sedas_client()\n",
    "    downloader = SeDASBulkDownload(sedas, down_dir, parallel=2)\n",
    "    downloader.add(sedas_res_dicts)\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Downloading\")\n",
    "    while not downloader.is_done():\n",
    "        time.sleep(5)\n",
    "    downloader.shutdown()\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_extract(down_zip, scene_dir):\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Extracting {down_zip}\")\n",
    "    with zipfile.ZipFile(down_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(scene_dir)\n",
    "    if os.path.exists(scene_dir):\n",
    "        os.remove(down_zip)\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Extracted to {scene_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud-Optimised Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert2cog(img_path, cog_path, band):\n",
    "    \"\"\"\n",
    "    Convert gdal raster into COG with default settings.\n",
    "    Considering lzw compression.\n",
    "    See https://www.cogeo.org/developers-guide.html.\n",
    "    \"\"\"\n",
    "    # translate into new cog file\n",
    "    kwargs = {\n",
    "        'format': 'COG',\n",
    "#         'creationOptions' : ['COMPRESS=LZW'],\n",
    "        'bandList': [band]\n",
    "    }\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Starting conversion: {img_path}.\")\n",
    "    ds = gdal.Translate(cog_path, img_path, **kwargs)\n",
    "    ds = None\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Conversion complete: {os.path.exists(cog_path)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mosaic2cog(imgs, out_cog, band):\n",
    "    \"\"\"\n",
    "    Mosaic imgs into cog.\n",
    "    Usual vrt assumptions.\n",
    "    \"\"\"\n",
    "    tmp_vrt = f\"{out_cog[:-4]}_mosaic.tif\"\n",
    "    # mosaic into vrt\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Mosaicing band {band} imgs: {imgs}\")\n",
    "    ds = gdal.BuildVRT(tmp_vrt, imgs, bandList=[band])\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Mosaicd {tmp_vrt}\")\n",
    "    # write vrt mosaic to cog\n",
    "    convert2cog(ds, out_cog, 1)\n",
    "    ds = None\n",
    "\n",
    "    if os.path.exists(tmp_vrt):\n",
    "        os.remove(tmp_vrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cogmosaicbands(img_paths, numbands, basename):\n",
    "    \"\"\"\n",
    "    Convert bands from img_paths to cog, output \n",
    "    basename_band#.tif. Mosaic if >1 img paths.\n",
    "    \"\"\"\n",
    "    if len(img_paths) == 1:\n",
    "        for b in range(1, numbands+1):\n",
    "            convert2cog(img_paths[0], f\"{basename}_band{b}.tif\", b)\n",
    "        for f in glob(f\"{img_paths[0][:-4]}*\"):\n",
    "            os.remove(f)\n",
    "    elif len(img_paths) > 1:\n",
    "        for b in range(1, numbands+1):\n",
    "            mosaic2cog(img_paths[0], f\"{basename}_band{b}.tif\", b)\n",
    "        for img_path in img_paths:\n",
    "            for f in glob(f\"{img_path[:-4]}*\"):\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_create_client(s3_bucket):\n",
    "    \"\"\"\n",
    "    Create and set up a connection to S3\n",
    "    :param s3_bucket:\n",
    "    :return: the s3 client object.\n",
    "    \"\"\"\n",
    "    access = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    secret = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "    session = boto3.Session(\n",
    "        access,\n",
    "        secret,\n",
    "    )\n",
    "    endpoint_url = os.getenv(\"AWS_S3_ENDPOINT_URL\")\n",
    "\n",
    "    if endpoint_url is not None:\n",
    "        s3 = session.resource('s3', endpoint_url=endpoint_url)\n",
    "    else:\n",
    "        s3 = session.resource('s3', region_name='eu-west-2')\n",
    "\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "    if endpoint_url is not None:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=access,\n",
    "            aws_secret_access_key=secret,\n",
    "            endpoint_url=endpoint_url\n",
    "        )\n",
    "    else:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=access,\n",
    "            aws_secret_access_key=secret\n",
    "        )\n",
    "    return s3_client, bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "gb = 1024 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_single_upload(in_path, s3_path, s3_bucket):\n",
    "    \"\"\"\n",
    "    put a file into S3 from the local file system.\n",
    "    :param in_path: a path to a file on the local file system\n",
    "    :param s3_path: where in S3 to put the file.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # prep session & creds\n",
    "    s3_client, bucket = s3_create_client(s3_bucket)\n",
    "\n",
    "    # Ensure that multipart uploads only happen if the size of a transfer is larger than\n",
    "    # S3's size limit for non multipart uploads, which is 5 GB. we copy using multipart \n",
    "    # at anything over 4gb\n",
    "    transfer_config = boto3.s3.transfer.TransferConfig(multipart_threshold=2 * gb,\n",
    "                                                       max_concurrency=10,\n",
    "                                                       multipart_chunksize=2 * gb,\n",
    "                                                       use_threads=True)\n",
    "    s3_client.upload_file(in_path, bucket.name, s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_rel_dir_s3_paths(local_dir, s3_dir):\n",
    "    \"\"\"\n",
    "    returns local path, remote path pair list.\n",
    "    \"\"\"  \n",
    "    paths = []\n",
    "    for subdir, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            paths.append([ full_path, s3_dir + local_dir.split('/')[-2] + '/' + full_path[len(local_dir):] ])\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_upload_dir(in_dir, s3_bucket, s3_dir):\n",
    "    \"\"\"\n",
    "    Upload all items in directory, inc. dir.\n",
    "    \"\"\"\n",
    "    paths = get_rel_dir_s3_paths(in_dir, s3_dir)\n",
    "    upload_list = [(in_path, out_path, s3_bucket)\n",
    "                   for in_path, out_path in paths]\n",
    "    for i in upload_list:\n",
    "        s3_single_upload(i[0], i[1], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_list_objects_paths(s3_bucket, prefix):\n",
    "    \"\"\"List of paths only returned, not full object responses\"\"\"\n",
    "    client, bucket = s3_create_client(s3_bucket)\n",
    "    \n",
    "    return [e['Key'] for p in client.get_paginator(\"list_objects_v2\").paginate(Bucket=s3_bucket, Prefix=prefix) for e in p['Contents']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_list_objects(s3_bucket, prefix):\n",
    "    # prep session & creds\n",
    "    client, bucket = s3_create_client(s3_bucket)\n",
    "    response = client.list_objects_v2(Bucket=s3_bucket, Prefix=prefix)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_up(work_dir):\n",
    "    # TODO: sort out logging changes...\n",
    "    gc.collect()\n",
    "    shutil.rmtree(work_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_logging():\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(os.environ.get(\"LOGLEVEL\", \"DEBUG\"))\n",
    "\n",
    "    # Turn down rasterio. It is extremely chatty at debug level.\n",
    "    logging.getLogger(\"rasterio\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"rasterio._io\").setLevel(\"WARNING\")\n",
    "\n",
    "    # Boto Core is also very chatty at debug. Logging entire request text etc\n",
    "    logging.getLogger(\"botocore\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"boto\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"boto3.resources\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"s3transfer\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"urllib3\").setLevel(\"INFO\")\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_snap_command(command, timeout =  60*45):\n",
    "    \"\"\"\n",
    "    Run a snap command. Internal use.\n",
    "\n",
    "    :param command: the list of arguments to pass to snap\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # if we need to prepend the snap executable.\n",
    "    if command[0] != os.environ['SNAP_GPT']:\n",
    "        full_command = [os.environ['SNAP_GPT']] + command\n",
    "    else:\n",
    "        full_command = command\n",
    "\n",
    "    # on linux there is a warning message printed by snap if this environment variable is not set.\n",
    "    base_env = os.environ.copy()\n",
    "    if \"LD_LIBRARY_PATH\" not in base_env and platform.system() != \"Windows\":\n",
    "        base_env[\"LD_LIBRARY_PATH\"] = \".\"\n",
    "\n",
    "    logging.debug(f\"running {full_command}\")\n",
    "\n",
    "    process = subprocess.Popen(full_command, env=base_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    process.timeout = timeout\n",
    "    snap_logger_out = logging.getLogger(\"snap_stdout\")\n",
    "    snap_logger_err = logging.getLogger(\"snap_stderr\")\n",
    "    std_out_reader = AsynchronousFileReader(process.stdout)\n",
    "    std_err_reader = AsynchronousFileReader(process.stderr)\n",
    "\n",
    "    def pass_logging():\n",
    "        while not std_out_reader.queue.empty():\n",
    "            line = std_out_reader.queue.get().decode()\n",
    "            snap_logger_out.info(line.rstrip('\\n'))\n",
    "        while not std_err_reader.queue.empty():\n",
    "            line = std_err_reader.queue.get().decode()\n",
    "            snap_logger_err.info(\"stderr:\" + line.rstrip('\\n'))\n",
    "    try:\n",
    "        while process.poll() is None:\n",
    "            pass_logging()\n",
    "\n",
    "        std_out_reader.join()\n",
    "        std_err_reader.join()\n",
    "    except subprocess.TimeoutExpired as e :\n",
    "        logging.error(f\"IGNORING subprocess timeout running {command}\")\n",
    "        return\n",
    "    if process.returncode != 0:\n",
    "        raise Exception(\"Snap returned non zero exit status\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1_prep_utils.ipynb.\n",
      "Converted Untitled.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
